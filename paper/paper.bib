@misc{WP3Website,
  title = {Wind Plant Performance Prediction (WP3) Benchmarking Project},
  howpublished = {\url{https://a2e.energy.gov/projects/wp3}},
  note = {Accessed 2020-01-26}
}

@inproceedings{McCann2018,
author = {McCann, John and Fields, Jason and Farren, Des and Hahn, Berthold and Mounir, Nadine},
file = {:Users/jperrsau/Documents/Mendeley Desktop/TEM92{\_}Wind Energy and Digitalization{\_}proceedings{\_}V02{\_}Final.pdf:pdf},
mendeley-groups = {NREL},
pages = {1--29},
title = {{Topical Expert Meeting {\#}92 on Wind Energy and Digitalization}},
year = {2018}
}

@article{lunacek2018,
  author={Monte Lunacek and M. Jason Fields and Anna Craig and Joseph C. Y. Lee and John Meissner and Caleb Philips and Shuangwen Sheng and Ryan King},
  title={Understanding Biases in Pre-Construction Estimates},
  journal={Journal of Physics: Conference Series},
  volume={1037},
  number={6},
  pages={062009},
  doi={10.1088/1742-6596/1037/6/062009},
  url={https://doi.org/10.1088/1742-6596/1037/6/062009},
  year={2018},
  abstract={The pre-construction energy generation of a wind farm (P50) is difficult to estimate and evaluate. This paper presents a methodology to measure the accuracy of the p50 prediction, which we call the Historical Validation Survey (HVS), for several wind farms in the continental United States. Our results indicate that there is a bias between predicted and measured energy, even when controlling for factors like grid curtailment and resource variability. We also find that our results depend on the assumptions we make during analysis, which we quantify with a sensitivity analysis. This method allows the estimation of uncertainty we have in our findings. When we account for reasonable ranges of model assumptions, we find that, in the most optimistic case, there is still a bulk −5.5% bias when estimating pre-construction energy generation. When controlling for grid curtailment this number reduces to a range of −3.5 to −4.5%.}
}

@techreport{NRELPhase1Report,
  author = {National Renewable Energy Laboratory},
  shortauthor = {NREL},
  year = {In progress},
  title = {Wind Plant Performance Prediction (WP3) Phase 1 Benchmark Report (In Draft)},
  institution = {Department of Energy}
}

@techreport{iec25,
  type = {Standard},
  key = {IEC 61400-25-2:2015},
  year = {2015},
  title = {Wind turbines – Part 25-2: Communications for monitoring and control of wind power plants – Information models},
  institution = {International Electrotechnical Commission}
}

@article{Bodini2020,
  author={Bodini, Nicola and Optis, Mike},
  title={Are Uncertainty Categories in a Wind Farm Annual Energy Production Estimate Actually Uncorrelated?},
  journal={Wind Energ. Sci.},
  volume={5},
  pages={1435--1488},
  doi={10.5194/wes-5-1435-2020},
  url={https://doi.org/10.5194/wes-5-1435-2020},
  year={2020},
  abstract={Calculations of annual energy production (AEP) from a wind farm – whether based on preconstruction or operational data – are critical for wind farm financial transactions. The uncertainty in the AEP calculation is especially important in quantifying risk and is a key factor in determining financing terms. Standard industry practice assumes that different uncertainty categories within an AEP calculation are uncorrelated and can therefore be combined through a sum of squares approach. In this analysis, we assess the rigor of this assumption by performing operational AEP estimates for over 470 wind farms in the United States. We contrast the standard uncertainty assumption with a Monte Carlo approach to uncertainty quantification in which no assumptions of correlation between uncertainty categories are made. Results show that several uncertainty categories do, in fact, show weak to moderate correlations, namely: wind resource interannual variability and the windiness correction (positive correlation), wind resource interannual variability and regression (negative), and wind speed measurement uncertainty and regression (positive). The sources of these correlations are described and illustrated in detail in this paper, and the effect on the total AEP uncertainty calculation is investigated. Based on these results, we conclude that a Monte Carlo approach to AEP uncertainty quantification is more robust and accurate than the industry standard approach.}
}


@book{Rossum2009,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}

@article{Pedregosa2011,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}

@inproceedings{Mckinney2010,
  title={Data structures for statistical computing in python},
  author={McKinney, Wes and others},
  booktitle={Proceedings of the 9th Python in Science Conference},
  volume={445},
  pages={51--56},
  year={2010},
  organization={Austin, TX},
  doi={10.25080/Majora-92bf1922-00a}
}

@book{oliphant2006guide,
  title={A guide to NumPy},
  author={Oliphant, Travis E},
  volume={1},
  year={2006},
  publisher={Trelgol Publishing USA}
}


@article{hunter2007matplotlib,
  title={Matplotlib: A 2D graphics environment},
  author={Hunter, John D},
  journal={Computing in science \& engineering},
  volume={9},
  number={3},
  pages={90--95},
  year={2007},
  publisher={IEEE},
  doi={10.1109/mcse.2007.55}
}


@misc{osti_1478526,
title = {Open OA, FKA: Wind Plant Performance Project (WP3) Benchmarking},
author = {Perr-Sauer, Jordan and Fields, Michael and Craig, Anna and Optis, Michael and Kemper, Travis and Sheng, Shuangwen and Meissner, John and Phillips, Caleb},
abstractNote = {A collection of software to support the WP3 benchmarking project, including (but not limited to) operational analysis, historical validation, and benchmarking code.},
url = {https://www.osti.gov/servlets/purl/1478526},
doi = {10.11578/dc.20181023.1},
year = {2018},
month = {2},
publisher = {USDOE Office of Energy Efficiency and Renewable Energy}
}


@article{Craig2018,
	doi = {10.1088/1742-6596/1037/5/052021},
	url = {https://doi.org/10.1088%2F1742-6596%2F1037%2F5%2F052021},
	year = 2018,
	month = {jun},
	publisher = {{IOP} Publishing},
	volume = {1037},
	pages = {052021},
	author = {Anna Craig and Mike Optis and Michael Jason Fields and Patrick Moriarty},
	title = {Uncertainty quantification in the analyses of operational wind power plant performance},
	journal = {Journal of Physics: Conference Series},
	abstract = {In the present work, we examine the variation introduced in the evaluation of an operating plant’s wind power production as a result of the choices analysts make in the processing of the operational data. For this study, an idealized power production for individual turbines over an operational period was predicted by fitting power curves to the turbine production data collected during expected operation (that is, without curtailment or availability losses). A set of 240 possible methods were developed for (a) defining what data represented expected operation and (b) modeling the power curve. The spread in the idealized power production as predicted by the different methods was on average almost 3% for the 100 turbines considered. Such significant variation places a lower bound on the precision with which analysts may employ such data as benchmarks for calibration of their energy estimation processes and limits the potential for identification of refinements to the energy estimation models for improved accuracy.}
}
